% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{report} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1.75cm} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options
\DeclareGraphicsExtensions{.png,.pdf,.jpg,.mps}
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage[english]{babel}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{braket}
\usepackage{esvect}
\usepackage{commath}
\usepackage{amsmath}

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\usepackage[none]{hyphenat}


\setcounter{secnumdepth}{4}

%%% END Article customizations

%%% The "real" document content comes below...

\title{\Huge Second year PhD status report}
\author{\LARGE Thomas Karl Warburton\\ \\ \normalsize Supervisor:\\Dr. Vitaly Kudryavtsev}


%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 


\begin{document}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand\bibname{References}
%\renewcommand\abstract{\Large\textbf{Abstract}\\ \normalsize}
\maketitle

\tableofcontents

\newpage 

\section*{Abstract}
A review of the progress made in the past year. Progress has been made in simulations for the 35 ton, where calorimetry has been tuned, tracking algorithms have been benchmarked, photon detector track matching been implemented and the initial workings for a proton identification method have been developed. Work has also been carried out on preparing a camera system for installation in the 35 ton as well as the incorporation of two muon generators into LArSoft. 

\section{Introduction to LArSoft}
All simulation work has been done in LArSoft which is a simulation, analysis and reconstruction package for Liquid Argon (LAr) Time Projection Chambers (TPC's) which is being used by many of the LAr-TPC's in operation, construction and planning within the US program {\cite{Church_LArSoft}. LArSoft has been developed to be detector agnostic, meaning that it can be used to simulate any detector geometry. To this end it is envisioned that it will be used as a platform for constant development for existing experiments through to those still in the planning phases such as DUNE. \\ 

LArSoft is built around the Fermilab-supported Analysis Reconstruction Framework (ART), which allows the full sequence of analysis, reconstruction and analysis to be built up in either a single events or a collection of events. External packages such as ROOT and GEANT4 are also incorporated into LArSoft meaning that the user does not have to co-ordinate specific versions of the packages which they want to use as the newest versions are automatically incorporated. \\

There are numerous mechanisms by which particles can be generated within the software. External packages which have already been incorporated into the software are GENIE, NuANCE and CRY. It is also possible to load in pre-made particles files made my user defined modules, or to use inbuilt single particle generation mode. The inbuilt particle generation is fully tunable as the momenta, positions and direction can be varied to encompass all space, along with the distribution of these quantities. For the studies presented here a combination of both CRY and user defined generation events have been used.\\

The co-ordinates and angles in LArSoft are defined as; 
\begin{itemize}
\item X - The beam direction (for the 35 ton prototype where there is no beam, positive x is in the opposite direction to that which electrons in the large TPC's drift), 
\item Y - The vertical direction, 
\item Z - Defined as such to have a right handed co-ordinate system.
\item $\theta$ - Angle between.
\item $\phi$ - Angle between.
\end{itemize}

\section{Tuning of calorimetry}
Correctly calculating the behaviour of different particles in the detector is an essential part of the simulation work for the 35 ton, therefore it is vital that the calorimetric corrections made are correct. One way of ensuring this is to make sure that the 'Minimally Ionising Particle' (MIP) peak for muons has a \( \frac{dE}{dx} \) value of 2.1 MeV cm\(^{-3}\). \\

To do this samples of 10,000 Anti-Muons have been simulated and reconstructed whenever there has been a change to the simulation framework. Using a modified box model \cite{ModBox} corrections are made to the measured \( \frac{dQ}{dx} \) on the wires to calculate a value for \( \frac{dE}{dx} \). Before this is done however, a conversion of ADC counts to number of electrons is made. This correction is crucial as when the detector properties or conditions change then the relationship between number of electrons per ADC count is likely to change and for all physics results it is essential to know the number of electrons produced which gave the observed effect. 

\section{Benchmarking of tracking algorithms}
Knowledge of the strengths and weaknesses of different tracking algorithms is vital when using them for physics analyses. To this end it is beneficial to develop a module which calculates and compares the efficiencies with which tracks are matched. Efficiencies are calculated for two simulated samples, an idealised sample of primary Anti-Muons, and a more realistic CRY sample. Both of which are outlined below. \\ 

The muons used in the idealised sample have cosmic ray like properties, whereby they had no minimum or maximum energy. The angular distribution of the muons follows a $\cos^2$ distribution and the positions follow a flat distribution in the X and Z directions, and a constant starting position in Y above the cryostat. \\

In the more realistic CRY sample particles are generated from a time of -1.6 ms to 16 ms which corresponds to a total of 11 drift windows, where a drift window is defined as the time it takes for an electron to drift from the Cathode Plane Assembly (CPA) to the Anode Plane Assembly (APA). This corresponds to a time of 1.6 ms for an electric field of 500 V cm\(^{-1}\), which is the nominal running field of the 35 ton phase II. A period of 10 drift windows or 'milliblock' was used as data from the 35 ton prototype is proposed to be read out in this way. Particles are also generated 1 drift window before the start of the milliblock as the electrons these particles produce could still be in the detector if they have not yet drifted to the APA's, as will be the case for real data. An important consideration for tracks caused by the particles which enter before, but cause tracks after T = 0 is that their initial positions in the cryostat have to be corrected in order to reflect their Monte Carlo positions at T = 0, so as to not calculate a Monte Carlo length which is not reconstructable.  

\subsection{Matching tracks with Monte Carlo information}
In order to match the Monte Carlo particle which induced a reconstructed track to the track a new data product was created. This required summing the charge deposited by each Monte Carlo particle at each reconstrcuted hit and then summing the charges deposited over all the hits assigned to the given track. The track is then matched to the Monte Carlo particle which contributed the most charge to the total charge collected in the track. \\

This means that the reconstructed quantities can then be compared to Monte Carlo quantities, so the accuracy of reconstruction can be measured. For example, it can be calculated how accurately the position at which the particle entered the active volume was reconstructed. This is a very important development for the simulation machinery.\\

This matching also solves a difficulty in reconstructing events with a large drift time through the identification of an interaction time. The reconstruction algorithms have to assume that the time at which charge is deposited on the APA's is related to the X position of where the ionisation electrons were produced. As a result, X positions of over 20 m can be reconstructed for events at the end of a milliblock sample. These large X positions obviously need to be corrected before analysis is performed and using Monte Carlo truth to correct these positions is a clearly viable option for simulated data.

\subsection{Calculating reconstructed efficiencies}
Accurately defining the metric used to measure efficiency is vital, as differing definitions could produce significantly different results. One such example is outlined below. \\

Suppose a particle travels 100 cm in the active volume of the detector, but is reconstructed as 2 separate tracks (tracks 1 and 2), with lengths 77 cm and 23 cm respectively. One metric of efficiency could be if the particle is reconstructed with a track length between \( 75\% \) and \( 125\% \) of the actual length traversed in the detector, in which case track 1 would be considered well reconstructed. One could however define efficiency as whether the distance traversed by the particle is between \( 75\% \) and \( 125\% \) of the reconstructed length, in which case neither track would be considered well matched. Both scenarios have used exactly the same tracks and a seemingly identical method of considering whether a track is well reconstructed or not, but have got the opposite results. As such, it would be unfair to say which consideration gave the correct result, but instead the result of each should be considered independantly. \\

For this study the first scenario is used, where a track is considered well reconsructed if it matches a well defined list of criteria;
\begin{itemize}
\item Reconstructed track length is more than or equal to \( 75\% \) of the Monte Carlo track length.
\item Reconstructed track length is less than or equal to \( 125\% \) of the Monte Carlo track length.
\item Only one reconstructed track can be matched per Monte Carlo particle.
\end{itemize}
Reconstructed efficiencies have been calculated for a number of particle sets. The most general of these sets considers all charged particles. A subset of this general set can also be considered so as to only consider certain particle types, such as muons or protons. \\

When calculating efficiencies it is important to consider much more than just the ratio of reconstructed to true track length. To this end efficiencies with regards to many aspects of the tracks are calculated;
\begin{itemize}
\item Track length
\item Energy deposited in the active volume of the detector
\item The angle $\theta$ of the track
\item The angle $\phi$ of the track
\end{itemize}
In all efficiency plots the Monte Carlo truth quantity, not the reconstructed quantity is shown so as to reflect how the variations of these quantities affect the reconstruction efficiencies. \\

Two sets of efficiencies are calculated for each reconstruction algorithm. One where reconstruction is performed as if the data were 'real' whereby no monte carlo truth information is used, and a second efficiency whereby Monte Carlo information is used to 'cheat' the hit disambiguation. This offers the chance to see the effect which disambiguation plays in reconstructing tracks. As expected cheating disambiugation improves the efficacy of the algorithms as it is less likely hits will be incorrectly reconstructed into clusters to seed the reconstructed tracks. Unless it is otherwise stated efficiencies referred to are those associated with the 'full' (non-cheated) reconstruction. 

\subsection{Status of tracking efficiencies}
From the idealised sample it was found that the three predominant reconstruction algorithms have significantly lower efficiencies for short particles than for longer particles, though this rapidly increases for tracks below 30 cm where the efficiencies all plateau at around 80-100 \%. This is because short tracks are difficult to reconstruct due to the low number of hits which they cause, and the large number of short tracks from particles such as low energy protons. There is however a significant drop in efficiency for tracks of length $\sim$195 cm, PMTrack for example exhibits a $\sim$30$\%$ drop in efficiency. This track length is approximately the vertical height of the detector and so any muons which travel vertically down (a large majority in a cosmic sample) would have this track length, this would cause all of the ionisation electrons to be incident on a single collection plane wire and thus make it very difficult to disambiguate hits on the other two planes. This is shown through comparison with the efficiency for cheated disambiguation where this drop is significantly less, $\sim$10$\%$ for PMTrack. \\

SHOW THE LENGTH PLOTS HERE! \\

It is also interesting to note the effect varying the angles $\theta$ and $\phi$ has on the efficiencies of the different reconstruction algorithms. This also offers verification that the drop in efficiencies for track of lengths is due to vertical muons and not merely a coincidence, as a similar drop in efficiency is also observed for $\theta$ $=$ $\frac{\pi}{2}$. \\

SHOW THE THETA VERSUS PHI PLOT COMP PLOT HERE \\

Efficiencies for the idealised simulation will continue to be calculated as they are useful for indentifying the absolute efficacy of reconstruction algorithms. However, more focus is placed on understanding the patterns in the milliblock sample as this sample more closely resembles what the data will look like as outlined above. \\

Initially when the milliblock sample was analysed the efficiencies were significantly lower as shown below. Only the efficiency as a function of length is shown to illustrate that this not a function of the milliblock having significantly more short, difficult particles to reconstruct but is in fact consistent for all track lengths. This decrease was only a significant problem when disambiguation was not cheated, which led to the realisation that disambiguation was only selecting the largest cluster in a given TPC. This had much more of an effect in the CRY sample where multiple particles enter the detector producing tracks than in the idealised sample where only a single particle enters the detector. \\

SHOW THE INITIAL MILLIBLOCK EFFICIENCY BEFORE DISAMBIG CORRECTION!!! \\

To correct this the disambiguation algorithm was restructured so as to select only clusters which were cleanly separated in channel and time space. Where a smaller cluster (determined by the number of hits) was fully contained within a large cluster, only the larger cluster was carried forward as it is assumed that this smaller cluster is due to misdisambiguated, fake hits. Following this improvement the efficiencies became comparable to the idealised efficiencies as shown below. \\

A slightly lower reconstruction efficiency is to be expected for the milliblock sample compared to the idealised sample due to its added complexity from having multiple primary particles at random times where co-incidence is likely. One feature which was not expected however is the decrease in efficiency which Pandora exhibits for tracks above 250 cm that is not repeated for the other algorithms. \\

SHOW THE MILLIBLOCK LENGTH PLOT \\

Hand-scanning of events showed that this was due tracks being stitched across the APA's at large times. As noted above, tracks at large times have large X offsets due to the reconstruction algorithms assumption that the ionisation electrons were produced at T = 0. Stitching tracks across the APA's at large times means that this X offset is doubled, causing tracks which are well reconstructed in Y and Z to have very large value for $\Delta$X, which causes the track length to be much more than 125\% of the Monte Carlo track length. The other algorithms do not do this as they check the change in X between TPC's, as well as the co-linearity in YZ. This is shown below, where a muon produced with a small time offset (T = 0.5 ms) crosses the APA and is stitched by Pandora but not by PMTrack. The stitching causes a large change in X, meaning the track length is significantly increased whilst the two tracks produced by PMTrack are not stitched and remain as two separate tracks. A drop in efficiency is not seen when the tracks are not stitched because for a long track a significant portion of the track (over 75\%) is likely to be in a single drift volume (usually the long drift volume), and only a small segment will be in the other drift volume (short drift volume). When this is the case, as only the relative length is considered when efficiency is calculated the long track will be considered 'well mathced' even though the bit of track in the other drift volume was not stitched. \\

These stitched tracks which are considered badly reconstructed are also evident in the $\theta$ vs $\phi$ plot shown below, where a large region of .................... space has a much lower efficiency for Pandora than for PMTrack or Cosmic Tracker. \\

SHOW THETA VS PHI MILLIBLOCK PLOT! \\

\section{Identifying interaction times from photon detectors}
In the 35 ton phase II design photon detectors are incorporated in the APA's as a means of assigning an interaction time (T0) for reconstructed tracks. Once a T0 is known, the X position of the track can be corrected to the hit time minus the flash time multiplied by the elctron drift velocity all divided by the electron drift velocity. Therefore, developing the code which evaluates the performance of these associations is another important step in preparing the detector for data taking. This also involved the first example of combining the TPC and photon detector simulations, which until this time had been totally separate. \\

An important step in this incorporation was adding the photon detector information to the LArSoft event display, most specifically the view in which the track positions are shown in the XZ and YZ planes. This is because when the flash is reconstructed the likely location of its origin is calculated and so being able to visually compare this position with track trajectories is particularly useful. \\

SHOW A SINGLE MUON TRACK WITH FLASH CLOSE TO IT!!! EVENT DISPLAY!!!\\

A similar method to that utilised in developing the tracking efficiencies was utilised in matching flashes with tracks, whereby an idealised simualtion of single anti-muons was initially used before extension to a CRY milliblock sample. Following hand-scanning of the anti-muon events two metrics for matching were established. \\

The strongest metric for matching was identified following the addition of the photon detector information to the event display, as it was observed that flashes are very well reconstructed in the YZ plane. This shows the excellent work performed by Alex Himmel (Fermilab) and Gleb Sinev (Duke) who led the photon detector reconstruction efforts. It was determined that using a Point of Closest Approach (PoCA) between the track space points and the flash centre was the best method of matching a flash to the track which caused it. This is compared to using the distance between the track and flash centres, or the distance between the flash centre and a line constructed between track start and end point below. This is because many tracks are not straight, and so stepping through the track space points is required. \\

COMPARE THE THREE WAYS OF MATCHING YZ SEPARATION HERE!!! \\

A second metric using the relationship between the number of photoelectrons collected in a flash and the true X position of the flash is also used. If two flashes have the same properties, but one is produced further away from the photon detectors then one would expect that the detected photons from the more distant flash would be more diffused and so would produce fewer photoelectrons. A fit of this relationship allows the X position of a flash to be predicted given the number of photoelectrons collected. Comparing this with the time separation of flashes and hits gives a method of predicting the X position of a flash. The difference in these predicted X positions is used as a metric. \\

SHOW THE PE vs X PLOT!!! \\

Using these two metrics it is possible to attempt to match a flash with a track. This is done by selecting tracks which occur within one drift window of the flash, and minimising the sum of these two metrics in quadrature. It is possible to weight the quantities to reflect that the $\Delta$YZ metric is more trusted than the $\Delta_X$ quantity. It is also possible that the absolute light levels and efficiencies of the photon detectors may be being incorrectly simulated and so this $\Delta_X$ quantity will not want to be used, at least initially. \\

SHOW THE TWO METRIC PLOT!!! \\

Simulations show that this method successfully matches tracks and flashes to within 1 ms, a large proportion of the time. This is shown through comparing the Monte Carlo truth time of the particle which caused the track and the flash time. This comparison also shows the simulated electronics response of the photon detectors to be 0.5 ms, as all successful matches have this offset. \\

SHOW THE MCTRUTH vs FLASH TIME, WHOLE AND ZOOM!!! 

\section{Development of a proton identification method}
Before proton identification can be performed using real data it is vitally important that a thorough simulation is performed to assess both the viability of the study and its proposed method. This has been performed using the large sample of CRY events used to measure tracking efficiency and to match photon flashes and tracks. \\

A preliminary study was performed to assess the predicted number of protons which would be seen per hour of data taking in the 35 ton detector. This was done by generating a CRY sample of length 1 hour and looking at the numbers of given particles which entered the active volume of the detector after the GEANT4 stage. This showed that roughly 40,000 protons would be produced per hour compared to almost 3 million muons, however when only stopping particles were considered the numbers became much more similar as almost all muons above 2 GeV are MIP's. \\

Subsequently a more refined estimation of reconstructed proton flux was performed. The reconstruction efficiency of the tracking algorithms for only protons was calculated so as to evaluate the number of well reconstructed protons that can be expected per hour of data taking. It should be noted that when correcting the X position of the track and also when performing calorimetric corrections the photon detector T0 is used. \\

SHOW THE PROTON LENGTH and THETA vs PHI EFFICIENCIES!!! \\

As PMTrack gives the best efficiencies further results shown are acquired using this reconstruction algorithm. ANY FURTHER DISCUSSION OF PLOTS? \\

Estimating the number of protons in the data is important, however they are still significantly outnumbered by muons in cosmic ray showers. Therefore it is neccessary to establish the mechanism by which a proton sample can be separated from a cosmic sample. One of the main mechanisms by which particle identification is performed in Liquid Argon (LAr) is using the relationship between $\frac{dE}{dx}$ and residual range. This relationship is particularly powerful near the end of the track, as described in \cite{PIDA} where a theoretical power-law dependence is used to identify protons in ArgoNeuT. Equation \ref{eq:PIDA_eq} shows the power law and the predicted $\frac{dE}{dx}$ versus residual ranges for different particle is shown in Figure \ref{fig:PIDA_Baller_MC}.

\begin{equation}
  \label{eq:PIDA_eq}
  \frac{dE}{dx}_{hyp} = A R^b 
\end{equation}

\begin{figure}[h]
  \centering
  \includegraphics[width=9cm]{PIDA_Pred_Baller.png}
  \caption{Plot of the $\frac{dE}{dx}_{hyp}$ versus residual range for different particles \cite{PIDA}.}
  \label{fig:PIDA_Baller_MC}
\end{figure}

Figure \ref{fig:PIDA_Baller_MC} shows that there is a weak dependence on b, but a strong dependence on A. This means that by setting b=-0.42 and calculating A for each spacepoint, it is possible to calculate an average A for a track. This average is referred to as PIDA and it's value for different particles is shown in shown in Figure \ref{fig:PIDA_Baller_PIDA}. \\

\begin{figure}[h]
  \centering
  \includegraphics[width=9cm]{PIDA_Baller_PIDA.png}
  \caption{Plot of the calculated PIDA for different particles, calculated with Monte Carlo Truth information \cite{PIDA}.}
  \label{fig:PIDA_Baller_PIDA}
\end{figure}

PIDA can only be calculated for stopping particles, as residual range calculations require a particle to stop within the detector. As such there are two methods by which this can be done, either cheating (using Monte Carlo information) or using reconstructed data. Only the former has been done thus far, work on using on reconstructed information will continue in parralel with other work. \\

A filter is applied to reconstructed Monte Carlo data to select only particles which stop in the detector, this should remove the MIP peak for particles which do not stop in the detector, and leave only tracks which terminate where the particle stops. However, as alluded to where tracking efficiencies were calculated, one can imagine the case where a long muon stops in the detector but is reconstructed as two separate tracks. In this case one of the partial tracks would end whilst the muon is still a MIP and so the $\frac{dE}{dx}$ versus residual range of this track would not increase as the track ends. This results in there still being a MIP peak in the cheated data sets, considered below. \\



\section{Incorporating muon generators}
Two muon generators have been incorporated into LArSoft for use by both the DUNE collaboration and wider community which use LArSoft. A muon generator for surface fluxes developed by Joel Klinger, referred to as Gaissers parameterisation module and a muon generator for underground fluxes developed by Vitaly Kudryavtsev called MUSUN. The details of both are outlined below.

\subsection{Gaissers parameterisation module}


\subsection{MUSUN}



\section{Status of camera modules in the 35 ton prototype}
Following work done in the previous year the camera system was sent to Fermilab to await installation into the 35 ton cryostat. To comply with cleanliness standards full cleaning of the all components using an ultrasound cleaner and alcohol was required. This was performed in November, along with a successful test installation of one camera onto one of the cryostat pipes where it was identified that the method for securing the modules was not substantial. Technicians at Fermilab agreed to use another method to attach them which they ensured would be sufficient for the modules. \\

Construction of the cold and warm cabling was also performed and labelled in preparation for its installation into the cryostat. Prior to installation it was neccessary to build the rack which would house the components for the monitoring system and check all the connections were still functionable. This required writing the documents for unattended use at Fermilab, which was granted in April. \\

At the time of writing all documentation has been produced and the camera system has been transfered to PC4 where the 35 ton resides and is awaiting installation within the coming weeks. Once installed the camera system will look at the high voltage (HV) breakdown in the cryostat. 

\newpage

\section{Thesis plan}
\begin{itemize}

\item Theory
  \subitem Neutrino oscillations
  \subitem Status of knowledge from experiments
  \subitem Liquid Argon detectors

\item Outline of LArSoft
  \subitem Structure and vision
  \subitem Geometries
  \subitem Simulations and analyses
  
\item The Deep Underground Neutrino Experiment (DUNE) detector
  \subitem Goals and status
  \subitem Modular design

\item The 35 ton prototype
  \subitem Phase I
  \subitem Phase II 
  
\item Service work for Phase II of the 35 ton prototype
  \subitem Simulations
  \subitem Shifts
  \subitem Hardware
  
\item The 35 ton camera system

\item Proton identification in the 35 ton prototype
  \subitem Basis for identifcation
  \subitem Simulations
  \subitem Identification from data
  
\item Cosmic background for the DUNE far detector
  \subitem Cosmic background for the LBNE surface detector
  \subitem Incorporation of generators into LArSoft
  \subitem Description of simulations and calculated backgrounds

\item Other work - proton decay?
    
\end{itemize}

\section{Thesis timetable}
Theory - Attendence at INSS will prove invaluable as many of the topics which will be in this section were covered there. In addition a reworking of the theory I did during my literature review last year will provide an excellent base at which to start. I would imagine this will be the first chapter to write, though the current experimental best fit data will need to be revisited upon completion.\\

Outline of LArSoft - An overview of how LArSoft is used and how it works as all simualtions and analyses showed in the thesis are done using LArSoft. Is distinct from theory, but still is an introductory part of the thesis and so can be written in early stages. \\

DUNE - Upon the completion of the CD1-refresh a clear statement is made of the design, capabilities and future plans for the experiment are made so this can be used to write this chapter. It is envisioned that this will provide an excellent base with which to write this chapter. \\

The 35 ton prototype - The workings of the prototype are known and papers and reference documents exist detailing the finer structure and results. These will be used to write this chapter, along with any future documents detailing the running of the phase II run. \\

Service work for the 35 ton - Lab book notes are being recorded for work and presentations being made as well as formal notes written. The service tasks performed are well integrated into the wider study of proton identification, and so these notes can likely be included with little change. Shifting will be recorded and observations noted for inclusion. Hardware work will also be recorded. \\

Camera system - Information for development and operation needs to be collated, will also include any results and papers which are written from running. \\

Proton identification - Monte Carlo studies are progressing and look promising, notes need to carry on being recorded. Identification from data will be done upon completion of data taking in early 2016. \\

Cosmic background and proton decay - Notes kept and written detailing the incorporation of the muon generation methods. No work has been done on calculating cosmic backgrounds for neutrino oscillation and no proton decay studies have been performed thus far, but upon implementation of work it will be recorded.

\section{List of conferences attended}
\begin{itemize}

\item Neutrino School for Theory-Experiment Collaboration (NuSTEC), Fermilab Oct 2014
  \subitem An interesting school devoted to neutrino cross section physics.
\item DUNE collaboration meeting, Fermilab, Jan 2015
\item DUNE collaboartion meeting, Fermilab, Apr 2015
\item International Neutrino Summer School (INSS), Sao Paulo, Aug 2015
  \subitem An interesting school covering a wide range of topics in neutrino physics, from its theoretical basis to current and future experimental capabilities. 
\item DUNE collaboartion meeting, Fermilab, Aug 2015

\end{itemize}

\section{Doctoral Development Programme}
Participation in the Research and Ethics and Integrity module was continued through a study group done remotely whereby example highlighting examples of good and poor ethics were found and discussed with other students. Continued development of theoretical knowledge was also achieved through attendance at conferences and schools. Development of computing knowledge was increased through further exposure, and a high level of competency has been gained with LArSoft due to its use every day. 

\begin{thebibliography}{56}
 
\bibitem{Church_LArSoft}
Eric Church, 
\emph{LArSoft: A Software Package for Liquid Argon Time Projection Drift Chambers},
\emph{arXiv:1311.6774v2}

\bibitem{Higgs}
CMS Collaboration,
\emph{Measurement of the properties of a Higgs boson in the four-lepton final state},
CMS-HIG-13-002, CERN-PH-EP-2013-220
\emph{arXiv:1312.5353v1}.

\bibitem{ModBox}
  S.Amoruso et al.,
\emph{Study of electron recombination in liquid argon with the ICARUS TPC},
Nucl. Instr. Meth. A, 523, 2004, 275.

\bibitem{PIDA}
R. Acciarri et al.,
  \emph{A study of electron recombination using highly ionizing particles in the ArgoNeuT Liquid Argon TPC}, 
  FERMILAB-PUB-13-184-E,
  \emph{arXiv:1306.1712}.

  
\end{thebibliography}
\end{document}
